Project Specification: Bit-Vox (8-Bit Speech Synth for Deluge)1. Project OverviewBit-Vox is a web-based speech synthesizer designed to create "Slice Kits" for the Synthstrom Deluge groovebox. It mimics the aesthetic of early 8-bit speech chips (specifically the C64's "Software Automatic Mouth" or SAM).The core workflow allows a user to type a sentence, break it into rhythmic syllables, assign a specific musical pitch to each syllable, and export a perfectly spaced .wav file that maps 1:1 to the Deluge's pad grid.2. Core Functional RequirementsA. Text-to-Syllable EngineInput: User types words or sentences (e.g., "ROBOT ACTIVATED").Phonetic Translation: The app must convert text to phonemes.Target Library: sam-js (or similar SAM port).Requirement: Access the internal phonetic translation of SAM so we can manipulate it.Syllabization (Crucial):The phoneme string must be broken into Syllables (e.g., "RO-BOT").Heuristic: Use vowel-cluster detection (Regex: /[^aeiouy]*[aeiouy]+(?:[^aeiouy](?![aeiouy]))*/gi) to group consonants with vowels.Result: An array of objects, where each object represents one "Slice" containing a list of phonemes.B. The Sequencer UIThe UI must visualize the sequence as a series of "Cards" or "Pads".Per-Syllable Controls:Phonemes: Editable list (e.g., [R], [OW]).Pitch: Musical Note (e.g., C3) and Octave.Duration: Slider to set length (in seconds).Global Controls:Preview: Play the sequence.Export: Generate the WAV.C. Audio Synthesis (SAM Integration)This is the primary upgrade from the prototype. instead of simple oscillators, use sam-js.The "Singing" Trick:Do not render the full sentence at once.Instantiate/Call SAM for each individual syllable.Pitch Control: Map the user's selected Note (Hz) to SAM's pitch parameter.Formula: SAM_Pitch = 22050 / Frequency_HzTimbre Controls: Expose SAM parameters per syllable (or globally):Speed (Duration)Pitch (Note)Throat (Formant shape)Mouth (Formant shape)D. Deluge Export Logic (Hardware Constraint)The Synthstrom Deluge "Slice" mode requires very specific file formatting to work automatically.Grid Alignment: The total number of slices in the WAV must be a multiple of 8 (8, 16, 32, 64).Logic: TargetCount = Math.ceil(Syllables.length / 8) * 8Uniform Spacing: Every slice in the file must be the exact same length.Logic: Find the Longest_Syllable_Duration. Pad all other syllables with silence to match this duration.Ghost Slices: If the user has 5 syllables, the export must append 3 "Silent" slices of the same duration to reach the target count of 8.3. Technical Stack RecommendationFramework: React (Vite recommended).Audio: Web Audio API (AudioContext, OfflineAudioContext).Synthesis Library: sam-js (via npm or local vendor file).State Management: React useState / useReducer is sufficient.4. Implementation Details for DeveloperStep 1: SAM WrapperCreate a wrapper around sam-js that accepts a phonetic string and a pitch value, and returns an AudioBuffer.// Pseudo-code for the singing wrapper
function renderPhonemes(phonemeString, noteFrequency) {
   const samPitch = 22050 / noteFrequency;
   // Configure SAM with pitch, speed, mouth, throat
   // Render to buffer
   return audioBuffer;
}
Step 2: The Render Loop (OfflineAudioContext)When exporting:Calculate maxDuration from the UI state.Calculate targetSliceCount (next multiple of 8).Create OfflineAudioContext with duration: targetSliceCount * maxDuration.Iterate through the syllables:Render the syllable audio using SAM.Place it at startTime = index * maxDuration.(Silence is automatically handled by the buffer gap).5. Prototype ReferenceCurrent UI: See speech-synth.jsx in the previous context for the visual layout and "Card" component structure.Current Logic: The autoSyllabize function in the prototype is robust and should be ported over.



Project File Structure & Asset Guide

1. Standard Vite/React Structure

The project should follow a standard Vite directory layout.

package.json:

Must include react, react-dom, and lucide-react.

index.html:

The entry point. Ensure the viewport meta tag is set for mobile responsiveness.

src/main.jsx:

The standard React DOM root render.

src/App.jsx:

THIS IS THE CORE FILE. The code from speech-synth.jsx in the prototype goes here.

It contains the UI, State, and the Sequencer logic.

2. The SAM Engine (The External Dependency)

Since sam-js is an external library (often found as a legacy script rather than a modern NPM package), you need to specify where it lives.

We recommend one of two approaches for the developer:

Approach A: The "Public" Folder (Easiest)

Place the SAM script files in the /public folder so they are accessible globally.

public/samjs.min.js (The core engine)

In index.html: Add <script src="/samjs.min.js"></script>

Approach B: The "Src/Lib" Folder (Modern)

If the developer uses a modular version of SAM:

src/lib/sam.js

In App.jsx: import SAM from './lib/sam';

3. Data Flow

Text Input -> App.jsx (Auto-Syllabizer) -> Syllable State

Syllable State -> App.jsx (Playback Loop) -> SAM Engine (AudioBuffer Generation)

SAM Engine -> App.jsx (Export Logic) -> WAV File

4. Key Functions to Preserve

When refactoring for sam-js, the developer must keep these logic blocks from the prototype:

autoSyllabize(word): The regex logic for splitting words is solid. Keep it.

handleExport(): The logic that calculates targetSliceCount (multiples of 8) is critical for Deluge compatibility. Do not change the math, only the audio source.
